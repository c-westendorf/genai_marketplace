---
description: Model evaluation metric requirements
globs: "**/*.py"
alwaysApply: false
version: "1.0.0"
author: "ml-team"
category: "ml"
tags: ["evaluation", "metrics", "validation"]
---

# Model Evaluation Metrics

Requirements for comprehensive model evaluation and reporting.

## Requirements

- Always compare to a baseline model (even a simple one)
- Report multiple metrics appropriate to the problem type
- Include confidence intervals for key metrics
- Document the evaluation dataset and split strategy
- Report metrics on the same held-out test set for fair comparison

## Patterns

### Good Pattern

```python
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix
)
from scipy import stats
import numpy as np

def evaluate_classification(
    y_true: np.ndarray,
    y_pred: np.ndarray,
    y_prob: np.ndarray,
    baseline_pred: np.ndarray,
    n_bootstrap: int = 1000
) -> dict:
    """Comprehensive classification evaluation with baseline comparison."""

    # Multiple metrics
    metrics = {
        "accuracy": accuracy_score(y_true, y_pred),
        "precision": precision_score(y_true, y_pred, average="weighted"),
        "recall": recall_score(y_true, y_pred, average="weighted"),
        "f1": f1_score(y_true, y_pred, average="weighted"),
        "roc_auc": roc_auc_score(y_true, y_prob, multi_class="ovr"),
    }

    # Baseline comparison
    baseline_metrics = {
        "baseline_accuracy": accuracy_score(y_true, baseline_pred),
        "baseline_f1": f1_score(y_true, baseline_pred, average="weighted"),
    }

    # Improvement over baseline
    metrics["accuracy_lift"] = metrics["accuracy"] - baseline_metrics["baseline_accuracy"]
    metrics["f1_lift"] = metrics["f1"] - baseline_metrics["baseline_f1"]

    # Confidence intervals via bootstrap
    metrics["f1_ci_95"] = bootstrap_ci(y_true, y_pred, f1_score, n_bootstrap)

    # Confusion matrix
    metrics["confusion_matrix"] = confusion_matrix(y_true, y_pred).tolist()

    return {**metrics, **baseline_metrics}


def bootstrap_ci(y_true, y_pred, metric_fn, n_bootstrap=1000, ci=0.95):
    """Calculate confidence interval via bootstrap."""
    scores = []
    n = len(y_true)
    for _ in range(n_bootstrap):
        idx = np.random.choice(n, n, replace=True)
        scores.append(metric_fn(y_true[idx], y_pred[idx], average="weighted"))

    lower = np.percentile(scores, (1 - ci) / 2 * 100)
    upper = np.percentile(scores, (1 + ci) / 2 * 100)
    return (lower, upper)
```

### Bad Pattern

```python
# Single metric, no baseline, no confidence intervals
def evaluate(y_true, y_pred):
    return {"accuracy": accuracy_score(y_true, y_pred)}
```

## Metric Selection Guide

| Problem Type | Primary Metrics | Secondary Metrics |
|-------------|-----------------|-------------------|
| Classification (balanced) | Accuracy, F1 | Precision, Recall, AUC |
| Classification (imbalanced) | F1, AUC-PR | Precision, Recall |
| Regression | RMSE, MAE | RÂ², MAPE |
| Ranking | NDCG, MRR | Precision@K |

## Exceptions

- Early exploratory analysis (but add comprehensive metrics before decisions)
- When specific business metric is the only requirement (document why)
